{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv,nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import  pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class keywords:\n",
    "    def __init__(self,name,definition,synonym):\n",
    "        self.name=name\n",
    "        self.definition=definition\n",
    "        self.synonym=synonym\n",
    "\n",
    "    def keywords_process(self):\n",
    "        word_arr = self.tokenizing()\n",
    "        stopwords_remove = self.stop_words_removal(word_arr)\n",
    "        count = self.freq(stopwords_remove)\n",
    "        parts_of_speech = self.parts_of_speech()\n",
    "        words = self.lemmatization(parts_of_speech, stopwords_remove)\n",
    "        # stemmed=self.stemming(stopwords_remove)\n",
    "        # print(stemmed)\n",
    "        # parts_of_speech=self.parts_of_speech()\n",
    "        # words=self.lemmatization(parts_of_speech,stopwords_remove)\n",
    "        # print(words)\n",
    "        # print(len(words))\n",
    "        return count, words\n",
    "\n",
    "    def pos_to_wordnet(self,pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        # elif pos_tag.startswith('M'):\n",
    "        #     return wordnet.MODAL\n",
    "        # elif pos_tag.startswith('R'):\n",
    "        #     return wordnet.ADVERB\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatization(self,part_of_speech,clean_words):\n",
    "        lemmatized = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        # print pos[2][0]\n",
    "        # print clean_words[1]\n",
    "        for i in range(0, len(part_of_speech)):\n",
    "            for j in range(0, len(clean_words)):\n",
    "                if part_of_speech[i][0].lower() == clean_words[j]:\n",
    "                    # print part_of_speech[i][0], clean_words[j], part_of_speech[i][1]\n",
    "                    lemmatized.append(\n",
    "                        (lemmatizer.lemmatize(clean_words[j], pos=self.pos_to_wordnet(part_of_speech[i][1]))).lower())\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "        # print(\"After Lemmatization No. of Words:\", len(list(set(lemmatized))))\n",
    "        return list(set(lemmatized))\n",
    "\n",
    "\n",
    "    def parts_of_speech(self):\n",
    "        parts_of_speech = pos_tag(word_tokenize(self.name+' '+self.definition+' '+self.synonym))\n",
    "        # print(parts_of_speech)\n",
    "        return parts_of_speech\n",
    "\n",
    "    def freq(self,words):\n",
    "        dict = {\n",
    "        }\n",
    "        for i in words:\n",
    "            dict[i] = words.count(i)\n",
    "        # print(\"Dictionary after stopwords removal(no lemmatization)\")\n",
    "        # print(dict)\n",
    "        return dict\n",
    "\n",
    "\n",
    "    def stemming(self,cleaning_words):\n",
    "        '''\n",
    "        The process of stemmimng is very dumb. Not always give reslutant output. The information passed through it might result in bad output.\n",
    "        Stemming is the process of finding the root word of the given word.\n",
    "        Prefer Lemmatization over it.\n",
    "        '''\n",
    "        ps = PorterStemmer()\n",
    "        # stem_words = ['play', 'playing', 'played', 'player', \"happy\", 'happier']\n",
    "        stemmed_words = [ps.stem(w) for w in cleaning_words]\n",
    "        # print(stem_words)\n",
    "        return stemmed_words\n",
    "\n",
    "    def stop_words_removal(self,word_arr):\n",
    "        # Stopwords and Punctuations.........................................\n",
    "        stop_words = stopwords.words('english')\n",
    "        # Stop Words are present of different languages, for papers of different languages.\n",
    "        # Cleaning words(removing stopwords and punctuations\n",
    "        # print(stop_words)\n",
    "        import string\n",
    "        punctuations = list(string.punctuation)\n",
    "        # print(string.punctuation)\n",
    "        stop_words += punctuations\n",
    "        # print(len(stop_words))\n",
    "        file_data = []\n",
    "        with open('D:\\Development\\Minor Project-Ontology Based\\Script\\Doctor Dataset Model\\stopwords', 'r') as f:\n",
    "            for line in f:\n",
    "                for word in line.split():\n",
    "                    file_data.append(word)\n",
    "        stop_words += file_data\n",
    "        stop_words = set(stop_words)\n",
    "        stop_words = list(stop_words)\n",
    "        new_word_arr = []\n",
    "        for i in word_arr:\n",
    "            new_word_arr.append(i.lower())\n",
    "        clean_words = [w for w in new_word_arr if not w in stop_words]\n",
    "        # print((stop_words))\n",
    "        # print(\"After cleaning the stopwords, no of words:\", len(clean_words))\n",
    "        return clean_words\n",
    "\n",
    "    def tokenizing(self):\n",
    "        # Tokenising..............................................\n",
    "        data=self.name+' '+self.definition+' '+self.synonym\n",
    "        # print(data)\n",
    "        word_arr =word_tokenize(data.lower())\n",
    "        #  Here lower case is done to remove more stopwords, but some information is lost\n",
    "        # print word_arr\n",
    "        return word_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('D:\\Development\\Minor Project-Ontology Based\\Script\\Doctor Dataset Model\\doid_updated.csv','r') as csvFile:\n",
    "    reader=csv.reader(csvFile)\n",
    "    data=list(reader)\n",
    "csvFile.close()\n",
    "feature_set=[]\n",
    "dataset=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,100):#len(data)):\n",
    "    # print(i)\n",
    "    obj=keywords(data[i][0],data[i][1],data[i][2])\n",
    "    dict_freq,words=obj.keywords_process()\n",
    "    feature_set+=[i for i in words]\n",
    "    dataset+=[(dict_freq,data[i][3])]\n",
    "# print(feature_set)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in feature_set:\n",
    "#     print(i)\n",
    "    for j in range(0,len(dataset)):\n",
    "        # print(list((dataset[j][0]).keys()))\n",
    "        if i in list(dataset[j][0].keys()):\n",
    "            dataset[j][0][i]= True\n",
    "            continue\n",
    "        else:\n",
    "            dataset[j][0][i] = False\n",
    "            # print(dataset[j][0].get(i))\n",
    "\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9c8958a2ef62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_y_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_y_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_y_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "x_y_train, x_y_test=dataset[0:700],dataset[700:]\n",
    "classifier=NaiveBayesClassifier.train(x_y_train)\n",
    "print('Accuracy:',nltk.classify.accuracy(classifier,x_y_test))\n",
    "print(classifier.show_most_informative_features(15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

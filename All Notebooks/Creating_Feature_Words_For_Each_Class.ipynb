{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv,nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import  pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('D:\\Development\\Minor Project-Ontology Based\\Sample_Doctor_Dataset\\CSV Files\\doid_updated.csv',\n",
    "          'r') as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    data = list(reader)\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_set_words:\n",
    "    def __init__(self, list_of_indices):\n",
    "        self.list_of_indices = list_of_indices\n",
    "\n",
    "    def building_dataset(self,words,output_class):\n",
    "        dataset=[]\n",
    "        temp={}\n",
    "        for j in range(0, len(data)):\n",
    "            temp={}\n",
    "            tokens = (self.tokenizing([data[j][1]]))\n",
    "            for i in range(0, len(tokens)):\n",
    "                if tokens[i] in words:\n",
    "                    temp[tokens[i]]=True\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            if data[j][i]==output_class:\n",
    "                dataset.append((temp,output_class))\n",
    "            else:\n",
    "                dataset.append((temp, \"Not \"+output_class))\n",
    "        return dataset\n",
    "    def changing_for_count_vectorization(self,words):\n",
    "        temp = []\n",
    "        sent = ''\n",
    "        for i in words:\n",
    "            for j in i:\n",
    "                sent = sent + ' ' + str(j)\n",
    "            temp.append(sent.strip())\n",
    "            sent = ' '\n",
    "        return temp\n",
    "\n",
    "    def feature_set_formation_for_incorrect_words(self):\n",
    "        final_words_for_count_vectorization = []\n",
    "        for i in range(1, len(data)):\n",
    "            if i in self.list_of_indices:\n",
    "                continue\n",
    "            else:\n",
    "                word_arr = self.tokenizing([data[i][1]])\n",
    "                stopwords_remove = self.stop_words_removal(word_arr)\n",
    "                parts_of_speech = self.parts_of_speech(data[i][1])\n",
    "                words = self.lemmatization(parts_of_speech, stopwords_remove)\n",
    "                final_words_for_count_vectorization.append(words)\n",
    "\n",
    "        temp = self.changing_for_count_vectorization(final_words_for_count_vectorization)\n",
    "        count_vec = CountVectorizer(max_features=10)\n",
    "        x_train_features = count_vec.fit_transform(temp[:])\n",
    "        # print(len(count_vec.get_feature_names()))\n",
    "        print((count_vec.get_feature_names()))\n",
    "        # print(x_train_features.todense())\n",
    "        return count_vec.get_feature_names()\n",
    "\n",
    "    def feature_set_formation_for_correct_words(self):\n",
    "        final_words_for_count_vectorization=[]\n",
    "        for i in range(1,len(self.list_of_indices)):\n",
    "            word_arr = self.tokenizing([data[i][1]])\n",
    "            stopwords_remove = self.stop_words_removal(word_arr)\n",
    "            parts_of_speech = self.parts_of_speech(data[i][1])\n",
    "            words = self.lemmatization(parts_of_speech, stopwords_remove)\n",
    "            final_words_for_count_vectorization.append(words)\n",
    "\n",
    "        # print(final_words_for_count_vectorization)\n",
    "        temp=self.changing_for_count_vectorization(final_words_for_count_vectorization)\n",
    "\n",
    "        count_vec = CountVectorizer(max_features=10)\n",
    "        x_train_features = count_vec.fit_transform(temp[:25])\n",
    "        # print(len(count_vec.get_feature_names()))\n",
    "        print((count_vec.get_feature_names()))\n",
    "        # print(x_train_features.todense())\n",
    "        return count_vec.get_feature_names()\n",
    "    def freq(self, words):\n",
    "        dict = {\n",
    "        }\n",
    "        for i in words:\n",
    "            dict[i] = words.count(i)\n",
    "        # print(\"Dictionary after stopwords removal(no lemmatization)\")\n",
    "        # print(dict)\n",
    "        return dict\n",
    "\n",
    "    def pos_to_wordnet(self, pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        # elif pos_tag.startswith('M'):\n",
    "        #     return wordnet.MODAL\n",
    "        # elif pos_tag.startswith('R'):\n",
    "        #     return wordnet.ADVERB\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemmatization(self, part_of_speech, clean_words):\n",
    "        lemmatized = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        # print pos[2][0]\n",
    "        # print clean_words[1]\n",
    "        for i in range(0, len(part_of_speech)):\n",
    "            for j in range(0, len(clean_words)):\n",
    "                if part_of_speech[i][0].lower() == clean_words[j]:\n",
    "                    # print part_of_speech[i][0], clean_words[j], part_of_speech[i][1]\n",
    "                    lemmatized.append(\n",
    "                        (lemmatizer.lemmatize(clean_words[j], pos=self.pos_to_wordnet(part_of_speech[i][1]))).lower())\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "        # print(\"After Lemmatization No. of Words:\", len(list(set(lemmatized))))\n",
    "        return list(set(lemmatized))\n",
    "\n",
    "    def parts_of_speech(self,definition):\n",
    "        parts_of_speech = pos_tag(word_tokenize(definition))  # +' '+self.synonym+ ' ' + self.name))\n",
    "        # print(parts_of_speech)\n",
    "        return parts_of_speech\n",
    "\n",
    "    def freq(self, words):\n",
    "        dict = {\n",
    "        }\n",
    "        for i in words:\n",
    "            dict[i] = words.count(i)\n",
    "        # print(\"Dictionary after stopwords removal(no lemmatization)\")\n",
    "        # print(dict)\n",
    "        return dict\n",
    "\n",
    "    def stemming(self, cleaning_words):\n",
    "        '''\n",
    "        The process of stemmimng is very dumb. Not always give reslutant output. The information passed through it might result in bad output.\n",
    "        Stemming is the process of finding the root word of the given word.\n",
    "        Prefer Lemmatization over it.\n",
    "        '''\n",
    "        ps = PorterStemmer()\n",
    "        # stem_words = ['play', 'playing', 'played', 'player', \"happy\", 'happier']\n",
    "        stemmed_words = [ps.stem(w) for w in cleaning_words]\n",
    "        # print(stem_words)\n",
    "        return stemmed_words\n",
    "\n",
    "    def stop_words_removal(self, word_arr):\n",
    "        # Stopwords and Punctuations.........................................\n",
    "        stop_words = stopwords.words('english')\n",
    "        # Stop Words are present of different languages, for papers of different languages.\n",
    "        # Cleaning words(removing stopwords and punctuations\n",
    "        # print(stop_words)\n",
    "        import string\n",
    "        punctuations = list(string.punctuation)\n",
    "        # print(string.punctuation)\n",
    "        stop_words += punctuations\n",
    "        # print(len(stop_words))\n",
    "        file_data = []\n",
    "        with open('D:\\Development\\Minor Project-Ontology Based\\Sample_Doctor_Dataset\\All Notebooks\\stopwords', 'r') as f:\n",
    "            for line in f:\n",
    "                for word in line.split():\n",
    "                    file_data.append(word)\n",
    "        stop_words += file_data\n",
    "        stop_words = set(stop_words)\n",
    "        stop_words = list(stop_words)\n",
    "        new_word_arr = []\n",
    "        for i in word_arr:\n",
    "            new_word_arr.append(i.lower())\n",
    "        clean_words = [w for w in new_word_arr if not w in stop_words]\n",
    "        # print((stop_words))\n",
    "        # print(\"After cleaning the stopwords, no of words:\", len(clean_words))\n",
    "        return clean_words\n",
    "\n",
    "    def tokenizing(self, parameters):\n",
    "        # Tokenising..............................................\n",
    "        data = '';\n",
    "        for i in parameters:\n",
    "            data += i + ' '\n",
    "        data = data.strip()\n",
    "        # print(data)\n",
    "        data = data.split('_')\n",
    "        # print(data)\n",
    "        sentence = ''\n",
    "        for i in data:\n",
    "            sentence += i\n",
    "        word_arr = word_tokenize(sentence.lower())\n",
    "        #  Here lower case is done to remove more stopwords, but some information is lost\n",
    "        # print word_arr\n",
    "        return word_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_y_train():\n",
    "    encoding_output = {}\n",
    "    c = 1\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i][3] in encoding_output.keys():\n",
    "            continue\n",
    "        else:\n",
    "            encoding_output[data[i][3]] = c\n",
    "            c = c + 1\n",
    "    # print(len(encoding_output))\n",
    "    # print(encoding_output)\n",
    "    y_train_test = []\n",
    "    for i in range(1, len(data)):\n",
    "        y_train_test.append(encoding_output[data[i][3]])\n",
    "    # print(y_train_test)\n",
    "    return y_train_test, encoding_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def creating_feature_set(output_class):\n",
    "    y_train_test,encoding_output=encoding_y_train()\n",
    "    # print(encoding_output)\n",
    "    temp_data=[]\n",
    "    # print(encoding_output['drug allergy'])\n",
    "    for i in encoding_output:\n",
    "        if i=='drug allergy':\n",
    "            for j in range(0,len(data)):\n",
    "                if i==data[j][3]:\n",
    "                    temp_data.append(j)\n",
    "                else:\n",
    "                    continue\n",
    "            print(temp_data)\n",
    "        else:\n",
    "            continue\n",
    "    feature_set=feature_set_words(temp_data)\n",
    "    words_correct=feature_set.feature_set_formation_for_correct_words()\n",
    "    words_incorrect=feature_set.feature_set_formation_for_incorrect_words()\n",
    "    incorrect_rows=[]\n",
    "    words_incorrect=set(words_incorrect)\n",
    "    for j in range(0,len(data)):\n",
    "        tokens=(feature_set.tokenizing([data[j][1]]))\n",
    "        for i in range(0,len(tokens)):\n",
    "            if tokens[i] in words_incorrect:\n",
    "                incorrect_rows.append(j)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    print(len(incorrect_rows))\n",
    "    dataset=feature_set.building_dataset(list(words_incorrect)+words_correct,'drug allergy')\n",
    "    print(dataset)\n",
    "\n",
    "    data_input = ['patent blue V allergy',\n",
    "              'A drug allergy that has_allergic_trigger patent blue V.',\n",
    "              'allergic contact dermatitis to DNP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 29, 30, 31, 32, 34, 35, 37, 38, 39, 67, 68, 72, 73, 78, 79, 80, 82, 84, 1540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abacavir', 'allergy', 'beta', 'cephalosporin', 'disease', 'drug', 'lactam', 'lidocaine', 'line', 'lymphatic']\n"
     ]
    }
   ],
   "source": [
    "creating_feature_set('abc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
